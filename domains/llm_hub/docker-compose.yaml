services:
  ollama:
    image: ollama/ollama:0.6.8
    container_name: ollama
    environment:
      - OLLAMA_KEEP_ALIVE=24h
    ports:
      - 11434:11434
    volumes:
      - type: bind
        source: ./scripts
        target: /scripts
    restart: unless-stopped
    entrypoint: ["env", "LLM_NAME=${LLM_NAME}", "/bin/sh", "/scripts/run_ollama.sh"]
    networks:
      - micro_agents_network

  api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: llm_hub_api
    depends_on:
      - ollama
    environment:
      - LLM_PROVIDER=${LLM_PROVIDER:-ollama}
      - LLM_URL=http://ollama:11434
      - LLM_NAME=${LLM_NAME:-llama3.2:1b}
    ports:
      - 8000:8000
    volumes:
      - type: bind
        source: ./api
        target: /module/api
    restart: unless-stopped
    entrypoint: ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--app-dir", "/module/api"]
    networks:
      - micro_agents_network

networks:
  micro_agents_network:
